{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfefd9e4",
   "metadata": {},
   "source": [
    "<H1> Desafio - SpaceTimeLabs </H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a7ca8-da46-4c60-b2b8-be8346e140dc",
   "metadata": {},
   "source": [
    "# Planejamento\n",
    "\n",
    "1. [x] Buscar dados: fontes [1] e [2];\n",
    "2. [x] Importar informações em DataFrame;\n",
    "3. [x] Tratar informações: extrair datas e normalizar quantidades;\n",
    "4. [X] Discernir entre quantidades por UF, região política, região econômica e país;  \n",
    "\n",
    "Esquema proposto para:\n",
    "\n",
    "Soja: \n",
    "```json\n",
    "    {\n",
    "        {{filename}}: {\n",
    "            'dataframe': pd.DataFrame, \n",
    "            'tempo': list, \n",
    "            'geodados': {\n",
    "                'Estados': {\n",
    "                    'dataframe': pd.DataFrame,\n",
    "                    'dict': dict\n",
    "                },\n",
    "                'Regiões políticas': {\n",
    "                    'dataframe': pd.DataFrame,\n",
    "                    'dict': dict\n",
    "                },\n",
    "                'Regiões econômicas': {\n",
    "                    'dataframe': pd.DataFrame,\n",
    "                    'dict': dict\n",
    "                }, \n",
    "                'País': {\n",
    "                    'dataframe': pd.DataFrame,\n",
    "                    'dict': dict\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n",
    "Clima: `pd.DataFrame`\n",
    "\n",
    "\n",
    "Dados estão disponíveis nos links abaixo:\n",
    "\n",
    "- Soja: https://www.conab.gov.br/info-agro/safras/serie-historica-das-safras/itemlist/category/911-soja\n",
    "- Clima: https://portal.inmet.gov.br/dadoshistoricos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55914f6-5626-44f2-85bd-70b645f5937a",
   "metadata": {},
   "source": [
    "## Preâmbulo - Importações, configurações e definições"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba70367-889e-45a9-969b-89234e402d7b",
   "metadata": {},
   "source": [
    "### Importações base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee5b85-1144-4a21-83fb-99aa7d9081ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getcwd, path, listdir\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "import ipytest\n",
    "import pytest\n",
    "\n",
    "import re\n",
    "import json\n",
    "import xlrd\n",
    "import csv\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import sys\n",
    "from typing import Union, List\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from locale import setlocale, atof, LC_ALL\n",
    "from locale import str as locale_str\n",
    "\n",
    "import plotly as plt\n",
    "import plotly.express as px\n",
    "from plotly.express import choropleth\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from zipfile import ZipFile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab479e1-1ea2-4312-bbed-51f60951d9bc",
   "metadata": {},
   "source": [
    "### Configurações base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22bc6df-b50f-42c2-9c56-896b5f8f29fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipytest.config.magics_flags = ['-qq']\n",
    "ipytest.autoconfig()\n",
    "\n",
    "# Define nível logging level para ERROR\n",
    "\n",
    "# Cria aformatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Cria logger e define nível para DEBUG\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a console handler and set the level to DEBUG\n",
    "ch = logging.StreamHandler(sys.stdout)\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    'ignore', \n",
    "    message=\"A value is trying to be set on a copy of a slice from a DataFrame.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f6613e-8437-4160-b429-542d3aeadae9",
   "metadata": {},
   "source": [
    "### Utilitários gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6ef69-647a-45db-b522-c921253c618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percorrer_e_mover_pastas(pasta_raiz, eh_verbose=False):\n",
    "    # Obter a lista de todas as subpastas\n",
    "    subpastas = [f.path for f in os.scandir(pasta_raiz) if f.is_dir()]\n",
    "    \n",
    "    # Percorrer cada subpasta\n",
    "    for subpasta in subpastas:\n",
    "        # Usar a subpasta como nova pasta raiz\n",
    "        percorrer_e_mover_pastas(subpasta)\n",
    "        \n",
    "        # Obter a lista de todos os arquivos na subpasta\n",
    "        arquivos = [f.path for f in os.scandir(subpasta) if f.is_file()]\n",
    "        \n",
    "        # Mover cada arquivo para a pasta raiz atual\n",
    "        for caminho_arquivo in arquivos:\n",
    "            nome_arquivo = os.path.basename(caminho_arquivo)\n",
    "            novo_caminho_arquivo = os.path.join(pasta_raiz, nome_arquivo)\n",
    "            # Check if the file already exists in the destination directory\n",
    "            if os.path.exists(novo_caminho_arquivo):\n",
    "                # Rename the file\n",
    "                nome_arquivo, extensao = os.path.splitext(nome_arquivo)\n",
    "                contador = 1\n",
    "                while True:\n",
    "                    novo_nome_arquivo = f\"{nome_arquivo}_{contador}{extensao}\"\n",
    "                    novo_caminho_arquivo = os.path.join(pasta_raiz, novo_nome_arquivo)\n",
    "                    if not os.path.exists(novo_caminho_arquivo):\n",
    "                        break\n",
    "                    contador += 1\n",
    "\n",
    "                if(eh_verbose):\n",
    "                    print(f\"Arquivo renomeado para {novo_nome_arquivo}\")\n",
    "            \n",
    "            # Move the file to the destination directory\n",
    "            shutil.move(caminho_arquivo, novo_caminho_arquivo)\n",
    "            \n",
    "            if(eh_verbose):\n",
    "                print(f\"Arquivo movido de {caminho_arquivo} para {novo_caminho_arquivo}\")\n",
    "        \n",
    "        # Remover a subpasta\n",
    "        os.rmdir(subpasta)\n",
    "        if(eh_verbose):\n",
    "            print(f\"Subpasta removida: {subpasta}\")\n",
    "\n",
    "def download_e_salvar_zip(url, folder_path, file_name, eh_verbose=False):\n",
    "    # Cria a pasta se ela não existir\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    # Caminho completo do arquivo\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Baixa o arquivo\n",
    "    with urlopen(url) as response, open(file_path, 'wb') as file:\n",
    "        file.write(response.read())\n",
    "    \n",
    "    if(eh_verbose):\n",
    "        print(f\"Arquivo ZIP baixado e salvo em: {file_path}\")\n",
    "    \n",
    "    # Extrai o arquivo\n",
    "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "        # Obtém o nome da pasta para extrair o conteúdo do arquivo\n",
    "        folder_name = os.path.splitext(file_name)[0]\n",
    "        extract_path = os.path.join(folder_path, folder_name)\n",
    "        \n",
    "        zip_ref.extractall(extract_path)\n",
    "    \n",
    "    if(eh_verbose):\n",
    "        print(f\"Conteúdo do arquivo ZIP extraído em: {extract_path}\")\n",
    "    \n",
    "    # Deleta o arquivo ZIP\n",
    "    os.remove(file_path)\n",
    "    if(eh_verbose):\n",
    "        print(f\"Arquivo ZIP deletado: {file_path}\")\n",
    "\n",
    "def baixar_e_salvar_arquivo_xls(\n",
    "    url: str, \n",
    "    pasta_destino: str,\n",
    "    nome_arquivo: str\n",
    "):\n",
    "    # Cria a pasta se ela não existir\n",
    "    if not os.path.exists(pasta_destino):\n",
    "        os.makedirs(pasta_destino)\n",
    "    \n",
    "    # Obtém o nome do arquivo a partir da URL\n",
    "    caminho_arquivo = os.path.join(pasta_destino, nome_arquivo)\n",
    "    \n",
    "    # Baixa o arquivo\n",
    "    resposta = requests.get(url)\n",
    "    if resposta.status_code == 200:\n",
    "        with open(caminho_arquivo, 'wb') as arquivo:\n",
    "            arquivo.write(resposta.content)\n",
    "            \n",
    "        print(f\"Arquivo baixado e salvo em: {caminho_arquivo}\")\n",
    "\n",
    "        # Verifica se é um arquivo XLS ou XLSX usando regex\n",
    "        if re.search(r'\\.xls[x]?$', nome_arquivo):\n",
    "            # Abre o arquivo XLS\n",
    "            workbook = xlrd.open_workbook(caminho_arquivo)\n",
    "            # Exemplo de como você pode processar o conteúdo do arquivo XLS\n",
    "            for sheet_name in workbook.sheet_names():\n",
    "                sheet = workbook.sheet_by_name(sheet_name)\n",
    "                \n",
    "    else:\n",
    "        print(f\"Falha ao baixar o arquivo: {resposta.status_code}\")\n",
    "\n",
    "def quantile1(x):\n",
    "    return x.quantile(0.25)\n",
    "\n",
    "def median(x):\n",
    "    return x.quantile(0.50)\n",
    "\n",
    "def quantile3(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "def obter_estado_para_eregiao():\n",
    "    return inverter_dicionario(obter_eregiao_para_estado())\n",
    "\n",
    "def inverter_dicionario(dicio: dict):\n",
    "    dicio_novo = dict()\n",
    "    \n",
    "    for chave, valor in dicio.items():\n",
    "        if(isinstance(valor, list)):\n",
    "            for el in valor:\n",
    "                chaves_dicio_novo = list(dicio_novo.keys())\n",
    "                if(el in chaves_dicio_novo):\n",
    "                    warn(f'Elemento {el} já tem chave associada {chaves_dicio_novo[el]}. ')\n",
    "                else:\n",
    "                    dicio_novo[el] = chave\n",
    "\n",
    "        else:\n",
    "            emsg = 'Todos valores do dicionário devem ser listas!'\n",
    "            raise ValueError(emsg)\n",
    "\n",
    "    return dicio_novo\n",
    "\n",
    "def obter_chave_dict(ref_dict: dict, valor):\n",
    "    candidatos = []\n",
    "    \n",
    "    for chave, lista_elementos in ref_dict.items():\n",
    "        if(isinstance(lista_elementos, list)):\n",
    "            if(valor in lista_elementos):\n",
    "                candidatos.append(chave)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            emsg = 'Todos valores do dicionário-referencia devem ser listas!'\n",
    "            raise ValueError(emsg)\n",
    "\n",
    "    if(len(candidatos) == 0):\n",
    "        return None\n",
    "    if(len(candidatos) == 1):\n",
    "        return candidatos[0]\n",
    "    else:\n",
    "        emsg = 'Apenas uma chave para valor fornecido deve existir!'\n",
    "        raise ValueError(emsg)\n",
    "\n",
    "def igualdade_string_relaxada(str1: str, str2: str):\n",
    "    return str1.lower() == str2.lower()\n",
    "\n",
    "def normalizar_numeros(number_str):\n",
    "    # Set the locale to the user's default locale\n",
    "    setlocale(LC_ALL, '')\n",
    "    \n",
    "    # Parse the number-like string to a float\n",
    "    number = atof(number_str)\n",
    "    \n",
    "    # Format the float back to a string using the user's locale\n",
    "    normalized_number_str = locale_str(number)\n",
    "    \n",
    "    return normalized_number_str.replace(',', '.')\n",
    "\n",
    "def remover_caracteres_especiais(texto):\n",
    "    from unicodedata import normalize\n",
    "    import re\n",
    "    \n",
    "    # Normalize the text to decomposed form\n",
    "    text_normalizado = normalize('NFD', texto)\n",
    "    \n",
    "    # Use regex to remove non-alphanumeric characters and spaces\n",
    "    texto_limpo = re.sub(r'[^a-zA-Z0-9\\s]', '', text_normalizado)\n",
    "    \n",
    "    # Remove extra spaces and return the cleaned text\n",
    "    return ' '.join(texto_limpo.split())\n",
    "\n",
    "def extrair_sheets_de_arquivo_xls(nome_arquivo, pasta_destino, pasta_origem, rows_to_skip=0):\n",
    "    from xlrd import open_workbook\n",
    "    from csv import writer\n",
    "    \n",
    "    # Abre workbook\n",
    "    rota_arquivo_fonte = pasta_origem+'/'+nome_arquivo\n",
    "    workbook = open_workbook(rota_arquivo_fonte)\n",
    "    \n",
    "    # Obtem nomes de todos sheets no workbook\n",
    "    nomes_sheet = workbook.sheet_names()\n",
    "    \n",
    "    # Loop through each sheet and write its content to a CSV file\n",
    "    novo_nome_sheets = []\n",
    "    for nome_sheet in nomes_sheet:\n",
    "        # Get the sheet by name\n",
    "        sheet = workbook.sheet_by_name(nome_sheet)\n",
    "        \n",
    "        # Create a new CSV file for the current sheet\n",
    "        prefixo_sheet_normalizado = remover_caracteres_especiais(nome_sheet).lower()\n",
    "        novo_nome_sheet = f'{prefixo_sheet_normalizado}_soja.csv'\n",
    "        novo_nome_sheets.append(novo_nome_sheet)\n",
    "        \n",
    "        csv_file_path = path.join(pasta_destino, novo_nome_sheet)\n",
    "        \n",
    "        with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "            csv_writer = writer(csv_file)\n",
    "            \n",
    "            # Write the content from the sheet to the CSV file, skipping rows_to_skip\n",
    "            for row_idx in range(rows_to_skip, sheet.nrows):\n",
    "                row_data = []\n",
    "                for col_idx in range(sheet.ncols):\n",
    "                    cell_value = sheet.cell_value(row_idx, col_idx)\n",
    "                    row_data.append(cell_value)\n",
    "                \n",
    "                csv_writer.writerow(row_data)\n",
    "    \n",
    "        print(f\"Sheet '{nome_sheet}' exportada para '{csv_file_path}'\")\n",
    "\n",
    "    return novo_nome_sheets\n",
    "\n",
    "def cherry_place(lst: list, from_index:int, to_index: int):\n",
    "    # Remove element at index 2 (3) and store it in a variable\n",
    "    element = lst.pop(from_index)\n",
    "\n",
    "      # Insert the element at the specified index\n",
    "    lst.insert(to_index, element)\n",
    "    \n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac77d75-fd90-4b50-82f8-42bdc24e29de",
   "metadata": {},
   "source": [
    "#### Testes - Utilitários gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f21a918-cd34-4f03-a90d-1716595678c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest --disable-warnings --color=yes --log-cli-level=info\n",
    "\n",
    "def test_obter_chave_dict():\n",
    "    ref_dict = {\n",
    "        'key1': ['value1', 'value2'],\n",
    "        'key2': ['value3', 'value4'],\n",
    "        'key3': ['value5', 'value6']\n",
    "    }\n",
    "\n",
    "    # Test case: valid input\n",
    "    assert obter_chave_dict(ref_dict, 'value3') == 'key2'\n",
    "    \n",
    "    # Test case: invalid input (non-list value)\n",
    "    try:\n",
    "        obter_chave_dict(ref_dict, 'value1')\n",
    "    except ValueError as e:\n",
    "        assert str(e) == 'Apenas uma chave para valor fornecido deve existir!'\n",
    "\n",
    "    # Test case: invalid input (list value not in dictionary)\n",
    "    try:\n",
    "        obter_chave_dict(ref_dict, ['value7', 'value8'])\n",
    "    except ValueError as e:\n",
    "        assert str(e) == 'Apenas uma chave para valor fornecido deve existir!'\n",
    "\n",
    "    # Test case: invalid input (dictionary values are not lists)\n",
    "    ref_dict_invalid = {\n",
    "        'key1': 'value1',\n",
    "        'key2': 'value2'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        obter_chave_dict(ref_dict_invalid, 'value1')\n",
    "    except ValueError as e:\n",
    "        assert str(e) == 'Todos valores do dicionário-referencia devem ser listas!'\n",
    "\n",
    "def test_igualdade_string_relaxada():\n",
    "    assert igualdade_string_relaxada('abc', 'AbC')\n",
    "    assert not igualdade_string_relaxada('abcd', 'AbC')\n",
    "\n",
    "def test_normalizar_numeros():\n",
    "\n",
    "    # Test with a number-like string using a different locale\n",
    "    # Modify the number_str and expected_result according to your locale\n",
    "    number_str = '123.456,789'\n",
    "    expected_result = '123456.789'\n",
    "    assert normalizar_numeros(number_str) == expected_result\n",
    "\n",
    "def test_inverter_dicionario():\n",
    "    # Test with a dictionary containing lists as values\n",
    "    dicio = {'a': [1, 2, 3], 'b': [4, 5, 6]}\n",
    "    expected_result = {1: 'a', 2: 'a', 3: 'a', 4: 'b', 5: 'b', 6: 'b'}\n",
    "    assert inverter_dicionario(dicio) == expected_result\n",
    "\n",
    "    # Test with a dictionary containing a non-list value\n",
    "    dicio = {'a': [1, 2, 3], 'b': 4}  # 'b' has a non-list value\n",
    "    with pytest.raises(ValueError):\n",
    "        inverter_dicionario(dicio)\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_list():\n",
    "    return [1, 2, 3, 4, 5]\n",
    "\n",
    "def test_cherry_place_same_index(sample_list):\n",
    "    result = cherry_place(sample_list.copy(), 2, 2)\n",
    "    assert result == [1, 2, 3, 4, 5]\n",
    "\n",
    "def test_cherry_place_forward(sample_list):\n",
    "    result = cherry_place(sample_list.copy(), 2, 0)\n",
    "    assert result == [3, 1, 2, 4, 5]\n",
    "\n",
    "def test_cherry_place_backward(sample_list):\n",
    "    result = cherry_place(sample_list.copy(), 0, 2)\n",
    "    assert result == [2, 3, 1, 4, 5]\n",
    "\n",
    "def test_cherry_place_end(sample_list):\n",
    "    result = cherry_place(sample_list.copy(), 0, 4)\n",
    "    assert result == [2, 3, 4, 5, 1]\n",
    "    \n",
    "    result = cherry_place(sample_list.copy(), 0, 10)\n",
    "    assert result == [2, 3, 4, 5, 1]\n",
    "\n",
    "def test_cherry_place_out_of_bounds(sample_list):\n",
    "    with pytest.raises(IndexError):\n",
    "        cherry_place(sample_list.copy(), 10, 0)\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_series():\n",
    "    return pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "def test_quantile1(sample_series):\n",
    "    result = quantile1(sample_series)\n",
    "    assert result == 3.25\n",
    "\n",
    "def test_median(sample_series):\n",
    "    result = median(sample_series)\n",
    "    assert result == 5.5\n",
    "\n",
    "def test_quantile3(sample_series):\n",
    "    result = quantile3(sample_series)\n",
    "    assert result == 7.75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0292c89b-2d0e-43f2-a870-6c24d325761c",
   "metadata": {},
   "source": [
    "### Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68485a54-e273-4bfc-b081-6138395510cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUBUSERCONTENT_URL = 'https://raw.githubusercontent.com'\n",
    "ROTA_GEOMETRIA_BRASIL = '/codeforamerica/click_that_hood/master/public/data/brazil-states.geojson'\n",
    "URL_GEOMETRIA_BRASIL = f'{GITHUBUSERCONTENT_URL}{ROTA_GEOMETRIA_BRASIL}'\n",
    "\n",
    "# Source: \n",
    "# https://python.plainenglish.io/how-to-create-a-interative-map-using-plotly-express-geojson-to-brazil-in-python-fb5527ae38fc\n",
    "def obter_geometria_brasil():\n",
    "    from json import load \n",
    "    \n",
    "    with urlopen(URL_GEOMETRIA_BRASIL) as response:\n",
    "        return load(response)\n",
    "    \n",
    "    return Brazil \n",
    "\n",
    "def obter_uf_para_estado(Brazil):\n",
    "    state_id_map = {}\n",
    "    for feature in Brazil['features']:\n",
    "        feature['id'] = feature['properties']['name']\n",
    "        sigla = feature['properties']['sigla']\n",
    "        state_id_map[sigla] = feature['id']\n",
    "    \n",
    "    return state_id_map\n",
    "\n",
    "def obter_regiao_para_estado():\n",
    "    return {\n",
    "        'NORTE': ['AC', 'AP', 'AM', 'PA', 'RO', 'RR', 'TO'],\n",
    "        'NORDESTE': ['AL', 'BA', 'CE', 'MA', 'PB', 'PE', 'PI', 'RN', 'SE'],\n",
    "        'CENTRO-OESTE': ['DF', 'GO', 'MT', 'MS'],\n",
    "        'SUL': ['PR', 'RS', 'SC'],\n",
    "        'SUDESTE': ['ES', 'MG', 'RJ', 'SP']\n",
    "    }\n",
    "\n",
    "def obter_estado_para_regiao():\n",
    "    return inverter_dicionario(obter_regiao_para_estado())\n",
    "\n",
    "def obter_eregiao_para_estado():\n",
    "    return {\n",
    "    'NORTE/NORDESTE': [\n",
    "        'AC', 'AP', 'AM', 'PA', 'RO', 'RR', 'TO', 'AL', 'BA', 'CE', 'MA', 'PB', 'PE', 'PI', 'RN', 'SE'\n",
    "    ],\n",
    "    'CENTRO-SUL': [\n",
    "        'DF', 'GO', 'MT', 'MS', 'PR', 'RS', 'SC', 'ES', 'MG', 'RJ', 'SP'\n",
    "    ]\n",
    "}\n",
    "\n",
    "GEOMETRIA_BRAZIL = obter_geometria_brasil()\n",
    "\n",
    "REGIAO_PARA_ESTADOS = obter_regiao_para_estado()\n",
    "ESTADOS_PARA_REGIAO = obter_estado_para_regiao()\n",
    "EREGIAO_PARA_ESTADOS = obter_eregiao_para_estado()\n",
    "ESTADOS_PARA_EREGIAO = obter_estado_para_eregiao()\n",
    "SIGLAS_PARA_ESTADOS = obter_uf_para_estado(GEOMETRIA_BRAZIL)\n",
    "\n",
    "GEOMARCADORES = [\n",
    "    'Estados', 'Regiões políticas', 'Regiões econômicas', 'País'\n",
    "]\n",
    "\n",
    "CLIMA_COLUNAS_TEMPO = [\n",
    "    'DATA (YYYY-MM-DD)', 'Data'\n",
    "]\n",
    "\n",
    "CLIMA_COLUNAS_DADOS = [\n",
    "    'PRECIPITAÇÃO TOTAL, HORÁRIO (mm)', \n",
    "    'TEMPERATURA DO AR - BULBO SECO, HORARIA (°C)'\n",
    "]\n",
    "\n",
    "COLUNA_PRECIPITACAO = CLIMA_COLUNAS_DADOS[0]\n",
    "COLUNA_TEMPERATURA = CLIMA_COLUNAS_DADOS[1]\n",
    "\n",
    "CLIMA_METRICS = {\n",
    "    COLUNA_PRECIPITACAO: 'sum',\n",
    "    COLUNA_TEMPERATURA: ['mean', 'std', quantile1, median, quantile3, 'min', 'max']\n",
    "}\n",
    "\n",
    "DELIMITADORES_TEMPO = ['-', '/']\n",
    "DELIMITADOR_CLIMA = ';'\n",
    "DELIMITADOR_SOJA = ','\n",
    "\n",
    "ENCODING_SOJA = 'utf8'\n",
    "ENCODING_CLIMA = 'latin-1'\n",
    "\n",
    "# URL do arquivo a ser baixado\n",
    "URL_DADOS_SOJA = \"https://www.conab.gov.br/info-agro/safras/serie-historica-das-safras/item/download/52220_02561b9dfaf9252623ee9876f592aaf4\"\n",
    "URL_DADOS_CLIMA = 'https://portal.inmet.gov.br/uploads/dadoshistoricos/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25fc161",
   "metadata": {},
   "source": [
    "## Questão 1\n",
    "\n",
    "Faça uma **análise histórica** da evolução anual dos valores de:\n",
    "\n",
    "1. **área plantada**; \n",
    "2. **produção**;\n",
    "3. **produtividade**. \n",
    "\n",
    "por estado brasileiro.\n",
    "\n",
    "Dados disponíveis em: https://www.conab.gov.br/info-agro/safras/serie-historica-das-safras/itemlist/category/911-soja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f091549e",
   "metadata": {},
   "source": [
    "### Utilitários de ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ad164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_geodados_soja(\n",
    "    df_: pd.DataFrame\n",
    "):\n",
    "    estados, regioes, regioes_economicas, pais = obter_geolocalizacoes(df_)\n",
    "    anos = obter_anos(df_)\n",
    "    \n",
    "    uf_marcador = df_.columns[0]\n",
    "    \n",
    "    mascara_tem_2 = df_[uf_marcador].str.len() == 2\n",
    "    \n",
    "    eh_regiao = df_[uf_marcador].isin(regioes)\n",
    "    eh_eregiao = df_[uf_marcador].isin(regioes_economicas)\n",
    "    eh_pais = df_[uf_marcador].isin(pais)\n",
    "    \n",
    "    mascara_uf = mascara_tem_2\n",
    "    mascara_regiao = eh_regiao\n",
    "    mascara_eregiao = eh_eregiao\n",
    "    mascara_brasil = eh_pais\n",
    "\n",
    "    df_soja_estados = df_[mascara_uf]\n",
    "    df_soja_regiao = df_[mascara_regiao]\n",
    "    df_soja_eregiao = df_[mascara_eregiao]\n",
    "    df_soja_pais = df_[mascara_brasil]\n",
    "    \n",
    "    uf_regiao = ['UF']+anos\n",
    "    coluna_regiao = ['region']+anos\n",
    "    coluna_eregiao = ['eregion']+anos\n",
    "    coluna_pais = ['country']+anos\n",
    "\n",
    "    df_soja_estados.columns = uf_regiao\n",
    "    df_soja_regiao.columns = coluna_regiao\n",
    "    df_soja_eregiao.columns = coluna_eregiao\n",
    "    df_soja_pais.columns = coluna_pais\n",
    "    \n",
    "    return df_soja_estados, df_soja_regiao, df_soja_eregiao, df_soja_pais\n",
    "    \n",
    "def tentar_conversao_float_coalescer_zero(\n",
    "    valor: str\n",
    "):\n",
    "    value = str(valor).strip()\n",
    "    return 0 if valor == '-' else float(valor)\n",
    "    \n",
    "def obter_marcador_serie(\n",
    "    df_: pd.DataFrame, \n",
    "    marcador: list\n",
    "):\n",
    "    coluna_descritiva = df_.columns[0]\n",
    "    coluna_nao_descritiva = df_.columns[1:]\n",
    "    mascara_marcador = df_[coluna_descritiva] == marcador\n",
    "    \n",
    "    df_marcador = df_[mascara_marcador]\n",
    "    \n",
    "    valores = []\n",
    "    for linha_id, linha in df_marcador.iterrows():\n",
    "        valores = [\n",
    "            tentar_conversao_float_coalescer_zero(linha[col])\n",
    "            for col in coluna_nao_descritiva\n",
    "        ]\n",
    "    \n",
    "    return valores\n",
    "    \n",
    "def obter_marcadores_serie(\n",
    "    df_: pd.DataFrame, \n",
    "    marcadores: list\n",
    "):\n",
    "    anos = [\n",
    "        int(valor) \n",
    "        for valor in df_.columns[1:]\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        marcador: obter_marcador_serie(df_, marcador) \n",
    "        for marcador in marcadores\n",
    "    } \n",
    "\n",
    "def obter_anos(\n",
    "    df_: pd.DataFrame\n",
    "):\n",
    "    return [\n",
    "        int(year.split('/')[0]) \n",
    "        for year in df_.columns[1:]\n",
    "    ]\n",
    "\n",
    "def obter_geolocalizacoes(\n",
    "    df_: pd.DataFrame\n",
    "):\n",
    "    uf_marcador = df_.columns[0]\n",
    "    \n",
    "    mascara_tem_2 = df_[uf_marcador].str.len() == 2\n",
    "    mascara_nao_tem_2 = df_[uf_marcador].str.len() != 2\n",
    "    \n",
    "    lista_regioes = list(df_[mascara_nao_tem_2][uf_marcador])\n",
    "    \n",
    "    estados = list(df_[mascara_tem_2][uf_marcador])\n",
    "    regioes = lista_regioes[0:5]\n",
    "    regioes_economicas = lista_regioes[5:7]\n",
    "    pais = [lista_regioes[7]]\n",
    "    \n",
    "    return estados, regioes, regioes_economicas, pais\n",
    "\n",
    "def carregar_dados_soja(\n",
    "    rota_absoluta_arquivo: str\n",
    "):\n",
    "    return pd.read_csv(\n",
    "        rota_absoluta_arquivo, \n",
    "        delimiter=DELIMITADOR_SOJA, \n",
    "        encoding=ENCODING_SOJA\n",
    "    )\n",
    "\n",
    "def transpor_geodados_soja(df_: pd.DataFrame):\n",
    "    df_ = df_.copy(deep=True)\n",
    "    \n",
    "    # 1. Seta coluna nao numerica como index\n",
    "    geo_index=[\n",
    "        col \n",
    "        for col in df_.columns \n",
    "        if not isinstance(col, int)\n",
    "    ][0]\n",
    "    \n",
    "    df_.set_index(geo_index, inplace=True)\n",
    "    \n",
    "    # 2. Transpoem DataFrame\n",
    "    df_ = df_.transpose()\n",
    "    \n",
    "    # 3. Reseta indice\n",
    "    df_.reset_index(inplace=True)\n",
    "    \n",
    "    # 4. Renomea 'UF' para 'anos'\n",
    "    df_.rename(columns={'index': 'anos'}, inplace=True)\n",
    "    \n",
    "    df_.columns.name = None\n",
    "\n",
    "    return df_\n",
    "\n",
    "def montar_geodados_soja(\n",
    "    geodado: pd.DataFrame, \n",
    "    geomarcadores: list\n",
    "):\n",
    "    df_geodados_soja = transpor_geodados_soja(geodado)\n",
    "    \n",
    "    return {\n",
    "        'dataframe': df_geodados_soja,\n",
    "        'dict': obter_marcadores_serie(geodado, geomarcadores)\n",
    "    }\n",
    "\n",
    "def montar_dados_soja(\n",
    "    df_: pd.DataFrame\n",
    "):\n",
    "    return {\n",
    "        geo_marcador: montar_geodados_soja(geodado, geolocalizacao)\n",
    "        for geo_marcador, \n",
    "            geodado, \n",
    "            geolocalizacao in zip(\n",
    "            GEOMARCADORES, \n",
    "            obter_geodados_soja(df_), \n",
    "            obter_geolocalizacoes(df_)\n",
    "        )\n",
    "    }\n",
    "\n",
    "def validar_nome_arquivo(\n",
    "    fname: str\n",
    "):\n",
    "    splitted_fname = fname.split('.')\n",
    "    return splitted_fname[-1].lower() == 'csv'\n",
    "\n",
    "def validar_nomes_arquivos(\n",
    "    nome_arquivos: list\n",
    "):\n",
    "    from numpy import where, array\n",
    "    \n",
    "    validation_arr = []\n",
    "    \n",
    "    for nome_arquivo in nome_arquivos:\n",
    "        validation_arr.append(validar_nome_arquivo(nome_arquivo))\n",
    "\n",
    "    are_false = array(validation_arr) == False\n",
    "    are_false_indexes = list(where(are_false)[0])\n",
    "    \n",
    "    if(len(are_false_indexes) != 0):\n",
    "        false_elems = [fnames[index] for index in are_false_indexes]\n",
    "\n",
    "        false_msg = '\\n'.join(false_elems)\n",
    "        error_message = f'Arquivos abaixo são inválidos:\\n{false_msg}'\n",
    "        \n",
    "        raise ValueError(error_message)\n",
    "\n",
    "def obter_dados_soja(\n",
    "    rota_origem: str, \n",
    "    nome_arquivos: list\n",
    "):\n",
    "    validar_nomes_arquivos(nome_arquivos)\n",
    "\n",
    "    dados_soja = dict()\n",
    "    \n",
    "    for nome_arquivo in nome_arquivos:\n",
    "        df_soja = carregar_dados_soja(rota_origem+nome_arquivo)\n",
    "        rota_arquivo = rota_origem+nome_arquivo\n",
    "        chave_ = nome_arquivo.split('.')[0]\n",
    "\n",
    "        dados_soja[chave_] = {\n",
    "            'dataframe': df_soja, \n",
    "            'tempo': obter_anos(df_soja), \n",
    "            'geodados': montar_dados_soja(df_soja)\n",
    "        }\n",
    "\n",
    "    return dados_soja\n",
    "\n",
    "def obter_geodados_por_geomarcador(dados_dict: dict, geomarcador: str):\n",
    "    dados_chaves = dados_dict.keys()\n",
    "\n",
    "    geodados_lista = []\n",
    "    for dados_coluna in dados_chaves:\n",
    "        df_ = dados_dict[dados_coluna]['geodados'][geomarcador]['dataframe']\n",
    "        geodados_lista.append(df_ )\n",
    "\n",
    "    return geodados_lista\n",
    "\n",
    "def plotar_geodados_interativo_soja(\n",
    "    geodados_lista: list,\n",
    "    titulo:str, subplot_titles: list, ylabels: list,\n",
    "    eh_empilhado: bool = True\n",
    "):\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(rows=1, cols=3, subplot_titles=subplot_titles)\n",
    "    \n",
    "    # Add stacked area plots to subplots\n",
    "    for idx, geodados_elem in enumerate(geodados_lista):\n",
    "        data_labels = [\n",
    "            col \n",
    "            for col in geodados_elem.columns \n",
    "            if col != 'anos'\n",
    "        ]\n",
    "\n",
    "        for enum_val, data_label in enumerate(data_labels):\n",
    "            if eh_empilhado:\n",
    "                fill_label = 'tozeroy' if enum_val == 0 else 'tonexty'\n",
    "            else:\n",
    "                fill_label = 'tozeroy'\n",
    "            \n",
    "            x = geodados_elem['anos']\n",
    "            y = geodados_elem[data_label]\n",
    "\n",
    "            scatter_data = go.Scatter(\n",
    "                x=x, y=y, \n",
    "                fill= fill_label, name=data_label, stackgroup='one'\n",
    "            )\n",
    "\n",
    "            fig.update_xaxes(title_text='Tempo [anos]', row=1, col=idx+1)\n",
    "            fig.update_yaxes(title_text=ylabels[idx], row=1, col=idx+1)\n",
    "            fig.add_trace(scatter_data, row=1, col=idx+1)\n",
    "\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(showlegend=True, hovermode='x', title=titulo)\n",
    "    \n",
    "    # Show plot\n",
    "    fig.show()\n",
    "\n",
    "def plotar_dados_soja(\n",
    "    marcador: str, \n",
    "    x: np.ndarray, \n",
    "    y_dict: dict, \n",
    "    y_marcador: str\n",
    "):\n",
    "    import matplotlib.pyplot as plt \n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Stacked area plot\n",
    "    plt.title('Acumulada')\n",
    "    plt.stackplot(x, y_dict.values(), labels=y_dict.keys())\n",
    "\n",
    "    plt.xlabel('Tempo [Anos]')\n",
    "    plt.ylabel(y_marcador)\n",
    "    plt.legend(title='Legend', loc='upper left')\n",
    "\n",
    "    plt.suptitle(f'{y_marcador} por {marcador}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411221e5-47dc-412d-9a75-402299cbb41c",
   "metadata": {},
   "source": [
    "### Extracao e transformação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a291ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your Excel file\n",
    "rota_raiz = getcwd()\n",
    "rota_pasta_soja = 'data/soja/'\n",
    "nome_arquivo_soja = 'SojaSerieHist.xls'\n",
    "\n",
    "rota_pasta = path.join(rota_raiz, rota_pasta_soja)\n",
    "rota_arquivo = path.join(rota_pasta, nome_arquivo_soja)\n",
    "\n",
    "baixar_e_salvar_arquivo_xls(URL_DADOS_SOJA, rota_pasta, nome_arquivo_soja)\n",
    "\n",
    "nome_arquivo = nome_arquivo_soja\n",
    "pasta_origem = rota_raiz+'/'+rota_pasta_soja\n",
    "pasta_destino = rota_raiz+'/'+rota_pasta_soja\n",
    "skip_rows = 5\n",
    "\n",
    "nome_arquivos = extrair_sheets_de_arquivo_xls(nome_arquivo_soja, pasta_origem, pasta_destino, skip_rows)\n",
    "dados_soja = obter_dados_soja(rota_pasta_soja, nome_arquivos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3537833-337c-4c64-8b47-6f1c6270715c",
   "metadata": {},
   "source": [
    "### Visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e79b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for geomarcador in GEOMARCADORES:\n",
    "    dfs = obter_geodados_por_geomarcador(dados_soja, geomarcador)\n",
    "    \n",
    "    ylabels = [\n",
    "        '[1000 hectares]', \n",
    "        '[kg/ha]', \n",
    "        '[1000 T]'\n",
    "    ]\n",
    "    \n",
    "    subplot_labels = [\n",
    "        'Área plantada de soja', \n",
    "        'Produtividade', \n",
    "        'Produção'\n",
    "    ]\n",
    "    \n",
    "    dfs = obter_geodados_por_geomarcador(dados_soja, geomarcador)\n",
    "    \n",
    "    titulo = geomarcador\n",
    "    \n",
    "    plotar_geodados_interativo_soja(dfs, titulo, subplot_labels, ylabels, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28083e68",
   "metadata": {},
   "source": [
    "## Questão 2\n",
    "\n",
    "Faça um **cruzamento dos dados históricos de produtividade de soja com dados históricos climáticos** (principalmente temperatura e precipitação). \n",
    "\n",
    "Avalie a existência de correlações entre fatores climáticos e produtividade de soja.\n",
    "\n",
    "Dados disponíveis em: https://portal.inmet.gov.br/dadoshistoricos\n",
    "\n",
    "Janela de tempo: [2000, 2024]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40598ae",
   "metadata": {},
   "source": [
    "### Utilitários de ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4b1928-4834-4ef0-b629-a3f2e476fa9f",
   "metadata": {},
   "source": [
    "#### Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca379579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "\n",
    "def obter_arquivos_clima(rota_clima):\n",
    "    from os import listdir\n",
    "    arquivos_clima = listdir(rota_clima)\n",
    "    \n",
    "    return [\n",
    "        arquivo_clima\n",
    "        for arquivo_clima in arquivos_clima\n",
    "        if igualdade_string_relaxada(\n",
    "            arquivo_clima.split('.')[-1], \n",
    "            'csv'\n",
    "        )\n",
    "    ] \n",
    "\n",
    "def obter_uf_pelo_arquivo_clima(climate_file_):\n",
    "    return climate_file_.split('_')[2]\n",
    "\n",
    "def obter_ufs_pela_rota_base(rota_clima):\n",
    "    arquivos_clima = obter_arquivos_clima(rota_clima)\n",
    "    \n",
    "    return list(\n",
    "        {\n",
    "            obter_uf_pelo_arquivo(arquivo_clima) \n",
    "            for arquivo_clima in arquivos_clima\n",
    "        }\n",
    "    )\n",
    "\n",
    "def obter_anos_pela_rota_base(rota_clima):\n",
    "    return [int(ano) for ano in listdir(rota_clima)]\n",
    "\n",
    "def normalizar_dataframe_clima(df_):\n",
    "    str_para_float = lambda x: float(str(x).replace(',', '.'))\n",
    "    normalizar_str = lambda x: float(str_para_float(normalizar_numeros(str(x))))\n",
    "    \n",
    "    df_[COLUNA_TEMPERATURA] = df_[COLUNA_TEMPERATURA].apply(str_para_float)\n",
    "    df_[COLUNA_PRECIPITACAO] = df_[COLUNA_PRECIPITACAO].apply(normalizar_str)\n",
    "\n",
    "    precipitacao_has_9999_float = df_[COLUNA_PRECIPITACAO] > 0\n",
    "    temperatura_has_9999_float = df_[COLUNA_TEMPERATURA] > -50\n",
    "\n",
    "    cleanse_mask = precipitacao_has_9999_float & temperatura_has_9999_float\n",
    "        \n",
    "    return df_[cleanse_mask]\n",
    "\n",
    "def obter_dados_clima(rota_fonte, skiprows=8):\n",
    "    df_ = pd.read_csv(\n",
    "        rota_fonte, \n",
    "        skiprows=skiprows,  \n",
    "        delimiter=DELIMITADOR_CLIMA, \n",
    "        encoding=ENCODING_CLIMA\n",
    "    )\n",
    "    \n",
    "    # Reset the index\n",
    "    df_.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return normalizar_dataframe_clima(df_)\n",
    "\n",
    "def adicionar_ano_a_dataframe_clima(df_: pd.DataFrame):\n",
    "    df = df_.copy()  # Create a copy of the DataFrame\n",
    "    col_tempo = df_.columns[0]\n",
    "    split_map = lambda x: float(re.split('|'.join(map(re.escape, DELIMITADORES_TEMPO)), str(x))[0])\n",
    "    df['ano'] = df_[col_tempo].apply(split_map)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def adicionar_uf_a_dataframe_clima(\n",
    "    df_: pd.DataFrame,\n",
    "    uf_clima: str\n",
    "):\n",
    "    df_['UF'] = uf_clima\n",
    "    df_len = len(df_.columns)\n",
    "    columns = list(df_.columns)\n",
    "    colunas_novas = cherry_place(columns, df_len-1, 2)\n",
    "    df_ = df_[colunas_novas]\n",
    "\n",
    "    return df_\n",
    "\n",
    "def remover_coluna_unnamed(\n",
    "    df_: pd.DataFrame\n",
    "):\n",
    "    coluna_unnamed = [ \n",
    "        col\n",
    "        for col in df_.columns\n",
    "        if col.lower().find('unnamed') != -1\n",
    "    ][0]\n",
    "    print(coluna_unnamed)\n",
    "    df_.drop(coluna_unnamed, axis=1)\n",
    "\n",
    "    return df_\n",
    "\n",
    "def transformar_dataframe_clima(\n",
    "    df_: pd.DataFrame,\n",
    "    arquivo_clima: str\n",
    "):\n",
    "    uf_clima = obter_uf_pelo_arquivo_clima(arquivo_clima)\n",
    "    df_ = adicionar_uf_a_dataframe_clima(df_, uf_clima)\n",
    "    \n",
    "    df_ = adicionar_ano_a_dataframe_clima(df_)\n",
    "    remover_coluna_unnamed(df_)\n",
    "    \n",
    "    return df_\n",
    "\n",
    "def agrupar_dataframe_clima(\n",
    "    df_: pd.DataFrame\n",
    "):\n",
    "    col_precipitacao = CLIMA_COLUNAS_DADOS[0]\n",
    "    col_temperatura = CLIMA_COLUNAS_DADOS[1]\n",
    "    \n",
    "    colunas_grupo = ['ano', 'UF']\n",
    "    \n",
    "    # Use the metrics dictionary in the groupby aggregation\n",
    "    result = df_.groupby(colunas_grupo).agg(CLIMA_METRICS)\n",
    "    \n",
    "    # Flatten the column index\n",
    "    result.columns = ['_'.join(col).strip() for col in result.columns.values]\n",
    "    \n",
    "    result = result.reset_index()\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0bcf67",
   "metadata": {},
   "source": [
    "### Extração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "rota_base_clima = getcwd() + '/data/clima/'\n",
    "anos_clima = obter_anos_pela_rota_base(rota_base_clima)\n",
    "\n",
    "rotas_clima = [\n",
    "    rota_base_clima+str(ano_clima) \n",
    "    for ano_clima in anos_clima\n",
    "] \n",
    "\n",
    "clima_anos_df = {\n",
    "    ano: None\n",
    "    for ano in anos_clima\n",
    "}\n",
    "\n",
    "total_arq = sum(\n",
    "    len(obter_arquivos_clima(rota_clima)) \n",
    "    for rota_clima in rotas_clima\n",
    ")\n",
    "\n",
    "print(f\"Total de arquivos a processar: {total_arq}\")\n",
    "\n",
    "i = 0\n",
    "ano_rota_clima_list = list(zip(anos_clima, rotas_clima))\n",
    "for ano_clima, rota_clima in ano_rota_clima_list:\n",
    "    arquivos_clima = obter_arquivos_clima(rota_clima)\n",
    "    \n",
    "    df_ano_clima = pd.DataFrame()\n",
    "    for arquivo_clima in tqdm(arquivos_clima):\n",
    "        rota_fonte_dados = path.join(rota_clima, arquivo_clima)\n",
    "        \n",
    "        try:\n",
    "            df_ = obter_dados_clima(rota_fonte_dados)\n",
    "            df_ = transformar_dataframe_clima(df_, arquivo_clima)\n",
    "            \n",
    "            df_ano_clima = pd.concat([df_, df_ano_clima])\n",
    "        except Exception as e:\n",
    "            logging.error(f'Erro na leitura do arquivo {rota_fonte_dados}: {e}', exc_info=True)\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    print(f'Total avaliado: {100*i/total_arq}')\n",
    "    \n",
    "    clima_anos_df[ano_clima] = df_ano_clima\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99b81f-4e0e-46c8-b74e-00c2a9539378",
   "metadata": {},
   "outputs": [],
   "source": [
    "anos = clima_anos_df.keys()\n",
    "\n",
    "clima_total_df = pd.DataFrame()\n",
    "\n",
    "for ano in anos:\n",
    "    clima_total_df = pd.concat([clima_total_df, clima_anos_df[ano]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b76d259-8183-474e-b864-5126de55421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_sumario_regiao_clima(\n",
    "    df_estados: pd.DataFrame    \n",
    "):\n",
    "    df_regiao = clima_total_df.copy()\n",
    "    para_regiao = lambda x: ESTADOS_PARA_REGIAO[x]\n",
    "    df_regiao['UF'] = df_regiao['UF'].apply(para_regiao)\n",
    "\n",
    "    return agrupar_dataframe_clima(df_eregiao)\n",
    "\n",
    "def obter_sumario_eregiao_clima(\n",
    "    df_estados: pd.DataFrame    \n",
    "):\n",
    "    df_eregiao = df_estados.copy()\n",
    "    para_eregiao = lambda x: ESTADOS_PARA_EREGIAO[x]\n",
    "    df_eregiao['UF'] = df_eregiao['UF'].apply(para_eregiao)\n",
    "\n",
    "    return agrupar_dataframe_clima(df_regiao)\n",
    "\n",
    "def obter_sumario_pais_clima(\n",
    "    df_estados: pd.DataFrame    \n",
    "):\n",
    "    df_pais = df_estados.copy()\n",
    "    df_pais['UF'] = 'Brasil'\n",
    "\n",
    "    return agrupar_dataframe_clima(df_pais)\n",
    "\n",
    "def obter_sumario_geoclima(\n",
    "    df_estados: pd.DataFrame\n",
    "):\n",
    "    return agrupar_dataframe_clima(df_estados), \\\n",
    "        obter_sumario_regiao_clima(df_estados), \\\n",
    "        obter_sumario_eregiao_clima(df_estados), \\\n",
    "        obter_sumario_pais_clima(df_estados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5418f69-4933-4ae0-8e19-1dc20c3a598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(GEOMARCADORES, obter_sumario_geoclima(clima_total_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6048f44-a605-4371-895b-391e4a0c67da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(GEOMARCADORES, obter_sumario_geoclima(clima_total_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df592d-19cb-40f4-ab66-41a0b3f7adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = getcwd()+'/data/clima/resumo_clima.csv'\n",
    "\n",
    "clima_total_df.to_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4490a7-6698-43e9-aebd-fe76891b1d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = getcwd()+'/data/clima/resumo_clima.csv'\n",
    "\n",
    "clima_total_df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d2424-782a-42ad-9b97-4fa92effe280",
   "metadata": {},
   "outputs": [],
   "source": [
    "anos = [2000, 2001, 2002, 2003]\n",
    "\n",
    "for ano in anos:\n",
    "    file_name = f'{ano}.zip'\n",
    "    folder_path = getcwd()+'/data/clima_test'\n",
    "    file_folder = folder_path+'/'+file_name.split('.')[0] \n",
    "    \n",
    "    url = URL_DADOS_CLIMA+file_name\n",
    "\n",
    "    download_e_salvar_zip(url, folder_path, file_name)\n",
    "    percorrer_e_mover_pastas(file_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc59bba-ac14-4a6c-bb00-22906967d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def montar_dados_clima(rota_clima):\n",
    "    df_soy = carregar_dados_soja(rota_clima)\n",
    "    anos, estados, regioes, eregioes, pais = obter_marcadores_dados(df_soy)\n",
    "    df_soy_states, df_soy_regions, df_soy_eregions, df_soy_country = obter_geodados(df_soy)\n",
    "    \n",
    "    return {\n",
    "        'estados': get_timeseries_labels(df_soy_states, states),\n",
    "        'regiões políticas': get_timeseries_labels(df_soy_regions, regions),\n",
    "        'regiões econômicas': get_timeseries_labels(df_soy_eregions, eregions),\n",
    "        'país': get_timeseries_labels(df_soy_country, country)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250cba11-2c24-46b1-bd54-8e803c7e7eeb",
   "metadata": {},
   "source": [
    "## Questão 3\n",
    "\n",
    "Elabore um ou mais painéis demonstrando, de forma resumida, as análises e conclusões dos itens anteriores. Recomenda-se que os painéis contenham elementos variados, como, por exemplo:\n",
    "\n",
    "- mapas coropléticos dos estados brasileiros, representando algumas das grandezas\n",
    "analisadas\n",
    "- gráficos de séries temporais\n",
    "- gráficos de distribuição\n",
    "\n",
    "Fonte: https://github.com/brunolnetto/dashoy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979b8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Certifique-se de ter o arquivo \"BR_UF_2021.shp\" em seu diretório ou ajuste o caminho conforme necessário\n",
    "shapes_path = getcwd()+\"/shapes/BR_UF_2022.shp\"\n",
    "estados = gpd.read_file(shapes_path)\n",
    "\n",
    "# Substitua \"data\" pelo seu DataFrame contendo os dados para a coropleta\n",
    "# Substitua \"column\" pelo nome da coluna contendo os dados a serem visualizados\n",
    "estados.plot(cmap=\"OrRd\", legend=True, figsize=(12, 8))\n",
    "\n",
    "plt.title(\"Unidades da federação\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce994ce-c5fc-4f02-a679-b5576affea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_id_map = {}\n",
    "for feature in Brazil['features']:\n",
    " feature['id'] = feature['properties']['name']\n",
    " sigla = feature['properties']['sigla']\n",
    " state_id_map[sigla] = feature['id']\n",
    "\n",
    "route='/nayanemaia/Dataset_Soja/main/soja%20sidra.csv'\n",
    "soybean = pd.read_csv(f'{githubusercontent_url}{route}')\n",
    "\n",
    "fig = choropleth(\n",
    " soybean,                                           #   soybean database\n",
    " locations = 'Estado',                              #   define the limits on the map/geography\n",
    " geojson = GEOMETRIA_BRAZIL,                        #   shape information\n",
    " color = \"Produção\",                                #   defining the color of the scale through the database\n",
    " hover_name = 'Estado',                             #   the information in the box\n",
    " hover_data = [\"Produção\", \"Longitude\", \"Latitude\"],\n",
    " title = \"Produtividade da soja (Toneladas)\",       #   title of the map\n",
    " animation_frame = 'ano'                            #   creating the application based on the year\n",
    ")\n",
    "\n",
    "fig.update_geos(fitbounds = \"locations\", visible = False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09884bad-f003-4b1f-8955-637219a3b84e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
